{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a026fdb-0015-4f5d-b200-28033bc507c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lseg.data as ld\n",
    "from lseg.data.content import historical_pricing\n",
    "from lseg.data.content.historical_pricing import Intervals\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ld.open_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87137076-ad0c-4e18-8d3d-51faf5010176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_as_csv(ticker,out_path, count=3000):\n",
    "    response = historical_pricing.summaries.Definition(\n",
    "        universe=ticker,\n",
    "        interval=Intervals.DAILY,\n",
    "        count=count,\n",
    "        fields=[\"BID\", \"ASK\"]\n",
    "    ).get_data()\n",
    "    \n",
    "    response.data.df.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f53a95-ae5b-4ef1-bcff-0c9918b7384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV containing the symbols and sectors\n",
    "symbols_df = pd.read_csv(\"sp500_symbols_sectors.csv\")\n",
    "symbols = symbols_df['Symbol'].tolist()\n",
    "errors = []\n",
    "\n",
    "# Loop through symbols with a progress bar\n",
    "for symbol in tqdm(symbols, desc=\"Processing symbols\"):\n",
    "    try:\n",
    "        save_data_as_csv(symbol, f'data_dl/{symbol}.csv')\n",
    "    except Exception as E:\n",
    "        try:\n",
    "            save_data_as_csv(f\"{symbol}.O\", f'data_dl/{symbol}.O.csv')\n",
    "        except Exception as E:\n",
    "            try:\n",
    "                save_data_as_csv(f\"{symbol}.K\", f'data_dl/{symbol}.K.csv')\n",
    "            except Exception as E:\n",
    "                errors.append(symbol)\n",
    "                print(f\"error on {symbol}\")\n",
    "\n",
    "# Print the errors\n",
    "print(f\"errors {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b15c09-6f81-4eb0-b3ff-32e750e6fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV containing the symbols and sectors\n",
    "symbols_df = pd.read_csv(\"sp500_symbols_sectors.csv\")\n",
    "symbols = symbols_df['Symbol'].tolist()\n",
    "industries = symbols_df['GICS Sector'].tolist()\n",
    "\n",
    "sym_ind = zip(symbols, industries)\n",
    "\n",
    "sym_ind_true = {}\n",
    "\n",
    "errors = []\n",
    "\n",
    "def lookup(symbol):\n",
    "    response = historical_pricing.summaries.Definition(\n",
    "        universe=symbol,\n",
    "        interval=Intervals.DAILY,\n",
    "        count=1,\n",
    "    ).get_data()\n",
    "\n",
    "# Loop through symbols with a progress bar\n",
    "for symbol, industry in tqdm(sym_ind, desc=\"Processing symbols\"):\n",
    "    try:\n",
    "        lookup(symbol)\n",
    "        sym_ind_true[symbol] = industry\n",
    "    except Exception as E:\n",
    "        try:\n",
    "            lookup(f\"{symbol}.O\")\n",
    "            sym_ind_true[f\"{symbol}.O\"] = industry\n",
    "        except Exception as E:\n",
    "            try:\n",
    "                lookup(f\"{symbol}.K\")\n",
    "                sym_ind_true[f\"{symbol}.K\"] = industry\n",
    "            except Exception as E:\n",
    "                errors.append(symbol)\n",
    "                print(f\"error on {symbol}\")\n",
    "\n",
    "# Print the errors\n",
    "print(f\"errors {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c992af-969b-4a9d-b0da-549b79113667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lseg.data.content import fundamental_and_reference\n",
    "\n",
    "definition = fundamental_and_reference.Definition(\n",
    "    universe=\"IBM.N\",\n",
    "    fields=[\"TR.Revenue\"]\n",
    ")\n",
    "\n",
    "definition.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e318e-87b7-4c7e-9429-2cb063e16127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing individual stock CSVs\n",
    "DATA_DIR = Path(\"data_dl\")\n",
    "BID_OUTPUT = \"aggregated_bid.csv\"\n",
    "ASK_OUTPUT = \"aggregated_ask.csv\"\n",
    "\n",
    "def get_stock_symbol(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the stock symbol from the filename.\n",
    "    Assumes filenames are in the format 'SYMBOL.csv'.\n",
    "    \"\"\"\n",
    "    return file_path.stem  # 'AAPL.csv' -> 'AAPL'\n",
    "\n",
    "def read_stock_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads a stock CSV and returns a DataFrame with 'Date' as datetime.\n",
    "    Ensures required columns are present.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "        if not {'Date', 'BID', 'ASK'}.issubset(df.columns):\n",
    "            raise ValueError(f\"Missing required columns in {file_path.name}\")\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading {file_path.name}: {e}\")\n",
    "\n",
    "def aggregate_stock_data():\n",
    "    \"\"\"\n",
    "    Aggregates BID and ASK data from individual stock CSVs into two separate CSVs.\n",
    "    Performs an outer join on dates to include all available dates.\n",
    "    Missing data for a stock on a particular date is represented as NaN.\n",
    "    \"\"\"\n",
    "    # Locate all CSV files in the data directory\n",
    "    csv_files = list(DATA_DIR.glob(\"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {DATA_DIR}\")\n",
    "    \n",
    "    # Initialize empty DataFrames for BID and ASK with 'Date' as the key\n",
    "    aggregated_bid_df = pd.DataFrame()\n",
    "    aggregated_ask_df = pd.DataFrame()\n",
    "    \n",
    "    # To keep track of errors\n",
    "    error_symbols = []\n",
    "    \n",
    "    # Process each CSV file with a progress bar\n",
    "    for file_path in tqdm(csv_files, desc=\"Aggregating stock data\"):\n",
    "        symbol = get_stock_symbol(file_path)\n",
    "        try:\n",
    "            df = read_stock_data(file_path)\n",
    "            \n",
    "            # Select relevant columns and rename them to the stock symbol\n",
    "            bid_series = df[['Date', 'BID']].rename(columns={'BID': symbol})\n",
    "            ask_series = df[['Date', 'ASK']].rename(columns={'ASK': symbol})\n",
    "            \n",
    "            # Merge with the aggregated DataFrames using an outer join on 'Date'\n",
    "            if aggregated_bid_df.empty:\n",
    "                aggregated_bid_df = bid_series\n",
    "                aggregated_ask_df = ask_series\n",
    "            else:\n",
    "                aggregated_bid_df = pd.merge(aggregated_bid_df, bid_series, on='Date', how='outer')\n",
    "                aggregated_ask_df = pd.merge(aggregated_ask_df, ask_series, on='Date', how='outer')\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_symbols.append(symbol)\n",
    "            print(f\"Error processing {symbol}: {e}\")\n",
    "    \n",
    "    # Sort the aggregated DataFrames by 'Date'\n",
    "    aggregated_bid_df = aggregated_bid_df.sort_values('Date').reset_index(drop=True)\n",
    "    aggregated_ask_df = aggregated_ask_df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Save the aggregated DataFrames as CSVs\n",
    "    aggregated_bid_df.to_csv(BID_OUTPUT, index=False)\n",
    "    aggregated_ask_df.to_csv(ASK_OUTPUT, index=False)\n",
    "    \n",
    "    print(f\"\\nAggregated BID data saved to {BID_OUTPUT}\")\n",
    "    print(f\"Aggregated ASK data saved to {ASK_OUTPUT}\")\n",
    "    \n",
    "    # Report any symbols that encountered errors\n",
    "    if error_symbols:\n",
    "        print(f\"\\nThe following symbols encountered errors and were skipped:\")\n",
    "        print(error_symbols)\n",
    "        # Optionally, save the errored symbols to a CSV for reference\n",
    "        pd.DataFrame(error_symbols, columns=[\"Symbol\"]).to_csv(\"error_symbols_aggregation.csv\", index=False)\n",
    "    else:\n",
    "        print(\"\\nAll symbols were processed successfully without errors.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    aggregate_stock_data()\n",
    "except Exception as e:\n",
    "    print(f\"Aggregation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe3950-0ab6-4c87-8035-45f0303d9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define file paths\n",
    "BID_FILE = \"aggregated_bid.csv\"\n",
    "ASK_FILE = \"aggregated_ask.csv\"\n",
    "\n",
    "# Load the aggregated CSVs into DataFrames\n",
    "aggregated_bid_df = pd.read_csv(BID_FILE, parse_dates=['Date'])\n",
    "aggregated_ask_df = pd.read_csv(ASK_FILE, parse_dates=['Date'])\n",
    "\n",
    "# Set 'Date' as the index for easier handling\n",
    "aggregated_bid_df.set_index('Date', inplace=True)\n",
    "aggregated_ask_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Calculate the percentage of missing values for each stock in BID and ASK datasets\n",
    "bid_missing_percentage = aggregated_bid_df.isna().mean() * 100  # Percentage per column\n",
    "ask_missing_percentage = aggregated_ask_df.isna().mean() * 100  # Percentage per column\n",
    "\n",
    "# Combine BID and ASK missing percentages into a single DataFrame for comparison\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'BID_Missing_Percentage': bid_missing_percentage,\n",
    "    'ASK_Missing_Percentage': ask_missing_percentage\n",
    "})\n",
    "\n",
    "# Calculate the maximum missing percentage across BID and ASK for each stock\n",
    "missing_data_summary['Max_Missing_Percentage'] = missing_data_summary[['BID_Missing_Percentage', 'ASK_Missing_Percentage']].max(axis=1)\n",
    "\n",
    "# Display the summary\n",
    "print(\"Missing Data Summary (First 5 Stocks):\")\n",
    "print(missing_data_summary.head())\n",
    "\n",
    "# Visualize missing data in BID dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(aggregated_bid_df.isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap - BID')\n",
    "plt.show()\n",
    "\n",
    "# Visualize missing data in ASK dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(aggregated_ask_df.isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap - ASK')\n",
    "plt.show()\n",
    "\n",
    "# Define the missing data threshold (e.g., 5%)\n",
    "MISSING_DATA_THRESHOLD = 15.0  # in percentage\n",
    "\n",
    "# Identify stocks exceeding the threshold in either BID or ASK\n",
    "stocks_to_remove = missing_data_summary[\n",
    "    (missing_data_summary['BID_Missing_Percentage'] > MISSING_DATA_THRESHOLD) |\n",
    "    (missing_data_summary['ASK_Missing_Percentage'] > MISSING_DATA_THRESHOLD)\n",
    "].index.tolist()\n",
    "\n",
    "print(f\"\\nNumber of stocks to remove: {len(stocks_to_remove)}\")\n",
    "print(f\"Stocks to remove: {stocks_to_remove}\")\n",
    "\n",
    "# Remove identified stocks from both BID and ASK datasets\n",
    "aggregated_bid_cleaned = aggregated_bid_df.drop(columns=stocks_to_remove)\n",
    "aggregated_ask_cleaned = aggregated_ask_df.drop(columns=stocks_to_remove)\n",
    "\n",
    "print(f\"\\nShape after removal - BID: {aggregated_bid_cleaned.shape}\")\n",
    "print(f\"Shape after removal - ASK: {aggregated_ask_cleaned.shape}\")\n",
    "\n",
    "# Handle remaining missing data using forward fill\n",
    "aggregated_bid_cleaned_ffill = aggregated_bid_cleaned.fillna(method='ffill')\n",
    "aggregated_ask_cleaned_ffill = aggregated_ask_cleaned.fillna(method='ffill')\n",
    "\n",
    "# Optionally, handle remaining NaNs with backward fill or interpolation\n",
    "# aggregated_bid_cleaned_bfill = aggregated_bid_cleaned_ffill.fillna(method='bfill')\n",
    "# aggregated_ask_cleaned_bfill = aggregated_ask_cleaned_ffill.fillna(method='bfill')\n",
    "# aggregated_bid_cleaned_interp = aggregated_bid_cleaned_ffill.interpolate(method='linear')\n",
    "# aggregated_ask_cleaned_interp = aggregated_ask_cleaned_ffill.interpolate(method='linear')\n",
    "\n",
    "# Drop any remaining NaN values (optional)\n",
    "aggregated_bid_final = aggregated_bid_cleaned_ffill.dropna()\n",
    "aggregated_ask_final = aggregated_ask_cleaned_ffill.dropna()\n",
    "\n",
    "print(f\"\\nFinal shape - BID: {aggregated_bid_final.shape}\")\n",
    "print(f\"Final shape - ASK: {aggregated_ask_final.shape}\")\n",
    "\n",
    "# Define output file paths\n",
    "CLEANED_BID_FILE = \"aggregated_bid_cleaned.csv\"\n",
    "CLEANED_ASK_FILE = \"aggregated_ask_cleaned.csv\"\n",
    "\n",
    "# Save the cleaned datasets\n",
    "aggregated_bid_final.to_csv(CLEANED_BID_FILE)\n",
    "aggregated_ask_final.to_csv(CLEANED_ASK_FILE)\n",
    "\n",
    "print(f\"\\nCleaned BID data saved to {CLEANED_BID_FILE}\")\n",
    "print(f\"Cleaned ASK data saved to {CLEANED_ASK_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4191a1-b611-4a61-8838-7896616212d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "BID_CLEANED_FILE = \"aggregated_bid_cleaned.csv\"\n",
    "ASK_CLEANED_FILE = \"aggregated_ask_cleaned.csv\"\n",
    "\n",
    "# Load the cleaned CSVs into DataFrames\n",
    "aggregated_bid_cleaned = pd.read_csv(BID_CLEANED_FILE, parse_dates=['Date'])\n",
    "aggregated_ask_cleaned = pd.read_csv(ASK_CLEANED_FILE, parse_dates=['Date'])\n",
    "\n",
    "# Visualize missing data in BID dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(aggregated_bid_cleaned.isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap - BID Cleaned')\n",
    "plt.show()\n",
    "\n",
    "# Visualize missing data in ASK dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(aggregated_ask_cleaned.isna(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap - ASK Cleaned')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7695491-c3a7-42e0-8ef1-0839ba066cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld.close_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
